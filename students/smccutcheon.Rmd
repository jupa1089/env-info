---
title: "Sarah McCutcheon Env-Info. Lab 2 & 3"
output: 
  html_document:
    toc: true
    toc_depth: 3
---
##Lab 2, HW 1


####Organization
[fire-mitigation](https://github.com/fire-mitigation)

### Content

```      
What is your burning environmental question that you would like to address? Feel free to provide group project, dissertation, and/or personal interest. What is the study area?
```


I would like to determine what California cities are most like Santa Barbara in regards to wildfire potential. I would like to create a weighted model, from expert opinion, to select cities based on the weighted factors. 

### Techniques
        
```
What techniques from the course do you think will be most applicable?
```

  I think the lessons on interactive plots and maps will be most useful to my specific question. I also believe, that the wrangling data and visualizing data lessons will be useful overall to my future career. 
        
### Data
        
```
What data have you already identified? Feel free to provide a link and/or details on the variables of interest.
```

  
  The data that we will use for our weighted model will come from two sources. One will be the various data layers that we have for GIS (e.g. vegetation, wildland urban interface, census, fire history, and jurisdictional layers). Our second data source will be a collective weight that is assigned to each of those variables based on the expert opinion. We are collecting that information on Thursday, January 21st from members of the Santa Barbara Fire Safe Council using the [analytical hierarchy process](https://en.wikipedia.org/wiki/Analytic_hierarchy_process).


####Our Process

1. Collect expert opinion on weighted factors using AHP survey
2. Collect and organize necessary map layers    
    + Get GIS layers
    + Clean them up and organize attribute table to show only the data that is needed
3. Combine the weighted factors and the layers in a GIS model to select the appropriate cities

  
###Image

```
2010 WUI Map
```
A map of the _most recent_ comprehensive calculation of total wildland urban intermix in the US is shown below. The map was created by the __US Forest Service__.

![](images/smccutcheon_2010_WUImap.png)

###Data File
```{r}
#read csv
d=read.csv('data/smccutcheon_FullData.csv')

#output summary
summary(d)
```


##Lab 3, HW 2


###Data Wrangling
```{r, eval=FALSE}

# list of packages
pkgs = c(
  'readr',        # read csv
  'readxl',       # read xls
  'dplyr',        # data frame manipulation
  'tidyr',        # data tidying
  'nycflights13', # test dataset of NYC flights for 2013
  'gapminder')    # test dataset of life expectancy and popultion

# install packages if not found
for (p in pkgs){
  if (!require(p, character.only=T)){
    install.packages(p)
  }
}

```

####Traditional csv read
```{r}

d = read.csv('../data/r-ecology/species.csv')
d
head(d)
summary(d)

```

####Using readr to read csv
```{r}

library(readr)

d = read_csv('../data/r-ecology/species.csv')
d
head(d)
summary(d)
```

#### Code using basic functions only
```{r}
#read in csv
surveys = read.csv('../data/r-ecology/surveys.csv')

# view data
head (surveys)
summary (surveys)

# limit columns to species and year
surveys_2 = surveys[,c('species_id', 'year')]

# limit rows to just species "NL"
surveys_3 = surveys_2[,c('species_id', 'year')]

# get count per year
surveys_4 = aggregate(species_id ~ year, data=surveys_3, FUN= 'length')

# write to csv
write.csv(surveys_4, 'data/surveys_smccutcheon.csv', row.names = FALSE)
```
####Code using nested functions
```{r}
# read in data
surveys = read.csv('../data/r-ecology/surveys.csv')

# view data
head(surveys)
summary(surveys)

# limit data with [], aggregate to count, write to csv
write.csv(
  aggregate(
    species_id ~ year, 
    data = surveys[surveys_2$species_id  == 'NL', c('species_id', 'year')], 
    FUN = 'length'), 
  'data/surveys_smccutcheon.csv',
  row.names = FALSE)
```
####Code using dplyr
```{r, message = FALSE}
# load libraries
library(readr)
library(dplyr)

# read in csv
surveys = read_csv('../data/r-ecology/surveys.csv') 

# dplyr elegance
surveys %T>%                          # note tee operator %T>% for glimpse
  glimpse() %>%                       # view data
  select(species_id, year) %>%        # limit columns
  filter(species_id  == 'NL') %>%     # limit rows
  group_by(year) %>%                  # get count by first grouping
  summarize(n = n()) %>%              #   then summarize
  write_csv('data/surveys_smccutcheon.csv') # write out csv
```

